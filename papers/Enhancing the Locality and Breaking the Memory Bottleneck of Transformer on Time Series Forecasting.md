# Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting

* Abstract
    * 本論文では、Transformerを用いて時系列の予測問題に取り組む。
    * Transformerを時系列の予測に適用するためには2つの大きな弱点がある。
        1. 局所性: Transformer の典型的なアーキテクチャにおける点単位のドット積自己組織化（Attention中の Query, Key Value の計算のとこなど）は、局所的な文脈の影響を受けないため、モデルが時系列の異常を受けやすくなる。
        1. メモリのボトルネック :Transformer の空間的な複雑さは、シーケンスの長さ L に対して二次的に増大する（Attentionの計算時やFeedFowardの部分でも）ため、長い時系列を直接モデル化することは不可能です。
    * これら 2 つの問題を解決するために、我々はまず、QueryとKeyを因果関係のある畳み込みで生成することで、注意メカニズムに局所的な文脈をよりよく組み込むことができる畳み込み自己注意を提案する。
    * 次に、メモリコストが O(L(logL) 2 )しかない LogSparse Transformer を提案し、細かい粒度と強い長期依存性を持つ時系列の予測精度を制約されたメモリ予算の下で向上させる。
    * 合成データと実世界のデータセットの両方で実験を行ったところ、最先端の技術と比較して良好な結果が得られた。
* Introduction
    * 時系列予測では状態空間モデルやARIMAモデルなどの統計モデリングがつかわれているが、各時系列を独立に扱う点やトレンド、季節性の成分調整にドメイン知識が必要になったりするので実用が難しい。
    * 最近はDNNもつかわれているが、LSTMやGRUなどは長期的なパターンの認識に弱いことが知られている。
    * 現実の時系列データでは長期パターンと短期パターンの両方が繰り返されることが多い。
    * Transformerは距離に関係なく入力データの任意の場所に着目できるため、長期的なパターンの認識に長けている可能性がある。
    * しかし、Transformer中の点単位のドット積自己組織化（Attention中の Query, Key Value の計算のとこなど）は、局所的な文脈の影響を受けないため、モデルが時系列の異常を受けやすくなる。
    * 本研究は、それらの課題に対してアプローチし、3つのことを実施した。
        1. Transformer アーキテクチャを時系列予測に適用することに成功し、合成データと実データの両方で大規模な実験を行い、RNN ベースのモデルよりも長期的な依存関係をうまく処理できる Transformer の潜在的な価値を検証した。
        1. self-Attention層でクエリとキーを生成するために因果的な畳み込みを採用することで、畳み込み自己記憶を提案した。
        形状などのローカルコンテキストを考慮したクエリとキーのマッチングにより、モデルの学習損失を低減し、予測精度をさらに向上させることができた。
        1. メモリのボトルネックを解消するために、わずか O(L(logL) 2 )の空間的な複雑さしか持たない LogSparse Transformer を提案した。
* Related Work
    * 時系列予測ではARIMAモデルをBox-Jenkins法で最適化したりするが、線形性を仮定したり各時系列データを独立に仮定したりするのでちょっと使いづらい。
    * 他には、時系列データを行列として取り扱い、それを因数分解することで予測を行う手法もある。
    * 階層ベイズ的なアプローチもある。
    * DNNでは、ARモデルとRNNを組み合わせる手法や、グローバルRNNを使う手法もある。
    * Transformerは自然言語処理や音声などで大きな成果を出しているが、先ほど述べた難点がありそのまま時系列データに適用することはできない。
* Background
    * この論文で取り扱うことに対して、いくつかの前提条件を定義する。
    * 互いに関連するN個の時系列データの集まりがあるとする。
        * このN個の時系列データは時間tまでの値がわかっているとする。
        * このN個の時系列データの$t+\tau$時点までの値を予測することを目標とする。
        * $t+\tau$時点までの時系列データの値はわからないが、$t+\tau$までの個々の時間はわかっているとする。
            * つまり、どの時点でデータが取得されるのかは事前に既知であるとする。
            * より単純な場合では、$t$と$t+1$の間の時間ステップは全て同じであると仮定する。
    * 予測モデルが一度学習されたあとは、予測する時はパラメータ$\Phi$はどの時点でも同じものを共有する。
    * Transformerのself-Attention機構の構造としては、multi-headのものを採用する。
        * これにより、個々のself-Attention機構が別々の短期的、長期的パターンを学習することを期待する。
    * Attentionの対象として将来の情報が選ばれることを防ぐために、softmaxの中で上三角要素をすべて-∞にするマスク行列を掛ける。
    * AttentionのFeedfowardでは2層の全結合層を使用し、活性化関数としてはReLUがつかわれる。
* Methodology
    * Enhancing the locality of Transformer
        * 時系列データの中の1点がパターンの一部なのか、変化点なのか、異常値なのかは前後の点との関係性に依存する。
        * そこで、AttentionのQVKの前に畳み込み層を入れることで、ある点と周囲の関係を利用できるようにする。
            * ある点の将来の情報を使わないようにするため、畳み込みではある点とその前の点のみを使って畳み込みを行うcausal convolutionを利用する。
            * 畳み込みのカーネルサイズ k = 1 のとき、これは通常のAttentionと同じになる。
            * QVKは別々の畳み込み層を使う。
                * そのため、QVKでそれぞれカーネルサイズを変えることも可能。
        * 元々のAttentionでもKeyに対しては全ての値が使われているので、Queryのところでも周囲情報が使えるということの意義が大きそう。
    * Breaking the memory bottleneck of Transformer
        * 普通のTransfomerの各Attention層の計算結果を見ると、最初の方の層ではAttentionは全体的に広がっているが、最後の方の層ではある箇所だけに強くAttentionが働き他の箇所ではほぼ0のスパースな結果になっている。
            * これは、Attention層ではスパースな計算方法を用いても性能が劣化しない可能性を示している。
        * そこで、Attentionの結果のj番目を計算する時に、入力のj番目のデータからlog_2の間隔で遡ったデータのみを使用することで計算量を減らせる。
        * さらに、Attention層をlog_2(系列長) + 1 以上にすることで全ての入力系列の点が少なくとも一回は計算に使用されることを証明できる。
        * 近くを重点的に見たい時は直近数点はlogに従わずに使うlocal Attentionやlog_2の間隔で取るのを複数回繰り返すRestart Attentionなども考えられる。
* Experiments
    * Synthetic datasets
        * Transformerがそもそも他の手法に比べて長期パターンを学習しやすいのかを検証した。
        * 大きい波を1回作ったあとに小さい波を複数回繰り返し、その後大きい波をもう一度繰り返すデータを作った。
        * これで、各手法で最後の波を予測できるかを評価した。
        * 評価指標では、$\rho$が0.5だと予測が正解より小さい場合も大きい場合もペナルティが同じで、$\rho$が大きいと予測が正解より小さい場合にペナルティが大きく、$\rho$が小さいと予測が正解より大きい場合にペナルティが大きくなる指標を使った。
        * 結果、LSTMベースの手法だと途中の波が長いと学習できないのに対し、Transformerは長くても学習できた。
    * Real-world datasets
        * 実世界のデータでも、Transformerに畳み込みとLogスパースを取り込んだ本論文の手法はLSTMベースや統計モデルベースの手法より精度が良かった。
        * また、畳み込み部分のカーネルサイズを大きくすると性能が良くなったことから、畳み込みは効果があると言える。
        * また、使用できるメモリサイズを固定した条件で通常のtransformerとLogスパースを入れたTransformerを比較すると、Logスパースを入れたほうが効果が大きい場合もあった。
            * 畳み込みが効いているデータセットとLogスパースが効いているデータセットがあるっぽい。
            * 局所的なパターンが重要なデータセットでは畳み込みが、長期的なパターンが重要な時はLogスパースが効くのかもしれない。
            * どっちが効くかはわからないので両方入れとけばいいといえばそう。
* Conclusion
    * Transformerに畳み込みとLogスパースを取り込んだ手法を開発した。
