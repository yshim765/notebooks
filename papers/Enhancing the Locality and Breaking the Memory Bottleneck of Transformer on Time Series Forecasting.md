# Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting

* Abstract
    * 本論文では、Transformerを用いて時系列の予測問題に取り組む。
    * Transformerを時系列の予測に適用するためには2つの大きな弱点がある。
        1. 局所性: Transformer の典型的なアーキテクチャにおける点単位のドット積自己組織化（Attention中の Query, Key Value の計算のとこなど）は、局所的な文脈の影響を受けないため、モデルが時系列の異常を受けやすくなる。
        1. メモリのボトルネック :Transformer の空間的な複雑さは、シーケンスの長さ L に対して二次的に増大する（Attentionの計算時やFeedFowardの部分でも）ため、長い時系列を直接モデル化することは不可能です。
    * これら 2 つの問題を解決するために、我々はまず、QueryとKeyを因果関係のある畳み込みで生成することで、注意メカニズムに局所的な文脈をよりよく組み込むことができる畳み込み自己注意を提案する。
    * 次に、メモリコストが O(L(logL) 2 )しかない LogSparse Transformer を提案し、細かい粒度と強い長期依存性を持つ時系列の予測精度を制約されたメモリ予算の下で向上させる。
    * 合成データと実世界のデータセットの両方で実験を行ったところ、最先端の技術と比較して良好な結果が得られた。
* Introduction
    * 時系列予測では状態空間モデルやARIMAモデルなどの統計モデリングがつかわれているが、各時系列を独立に扱う点やトレンド、季節性の成分調整にドメイン知識が必要になったりするので実用が難しい。
    * 最近はDNNもつかわれているが、LSTMやGRUなどは長期的なパターンの認識に弱いことが知られている。
    * 現実の時系列データでは長期パターンと短期パターンの両方が繰り返されることが多い。
    * Transformerは距離に関係なく入力データの任意の場所に着目できるため、長期的なパターンの認識に長けている可能性がある。
    * しかし、Transformer中の点単位のドット積自己組織化（Attention中の Query, Key Value の計算のとこなど）は、局所的な文脈の影響を受けないため、モデルが時系列の異常を受けやすくなる。
    * 本研究は、それらの課題に対してアプローチし、3つのことを実施した。
    1. Transformer アーキテクチャを時系列予測に適用することに成功し、合成データと実データの両方で大規模な実験を行い、RNN ベースのモデルよりも長期的な依存関係をうまく処理できる Transformer の潜在的な価値を検証した。
    1. self-Attention層でクエリとキーを生成するために因果的な畳み込みを採用することで、畳み込み自己記憶を提案した。
        形状などのローカルコンテキストを考慮したクエリとキーのマッチングにより、モデルの学習損失を低減し、予測精度をさらに向上させることができた。
    1. メモリのボトルネックを解消するために、わずか O(L(logL) 2 )の空間的な複雑さしか持たない LogSparse Transformer を提案した。
* Related Work
    * 