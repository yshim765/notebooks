# Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting

* Abstract
    * 本論文では、Transformerを用いて時系列の予測問題に取り組む。
    * Transformerを時系列の予測に適用するためには2つの大きな弱点がある。
        1. 局所性: Transformer の典型的なアーキテクチャにおける点単位のドット積自己組織化（Attention中の Query, Key Value の計算のとこなど）は、局所的な文脈の影響を受けないため、モデルが時系列の異常を受けやすくなる。
        1. メモリのボトルネック :Transformer の空間的な複雑さは、シーケンスの長さ L に対して二次的に増大する（Attentionの計算時やFeedFowardの部分でも）ため、長い時系列を直接モデル化することは不可能です。
    * これら 2 つの問題を解決するために、我々はまず、QueryとKeyを因果関係のある畳み込みで生成することで、注意メカニズムに局所的な文脈をよりよく組み込むことができる畳み込み自己注意を提案する。
    * 次に、メモリコストが O(L(logL) 2 )しかない LogSparse Transformer を提案し、細かい粒度と強い長期依存性を持つ時系列の予測精度を制約されたメモリ予算の下で向上させる。
    * 合成データと実世界のデータセットの両方で実験を行ったところ、最先端の技術と比較して良好な結果が得られた。
* Introduction
    * 時系列予測では状態空間モデルやARIMAモデルなどの統計モデリングがつかわれているが、各時系列を独立に扱う点やトレンド、季節性の成分調整にドメイン知識が必要になったりするので実用が難しい。
    * 最近はDNNもつかわれているが、LSTMやGRUなどは長期的なパターンの認識に弱いことが知られている。
    * 現実の時系列データでは長期パターンと短期パターンの両方が繰り返されることが多い。
    * Transformerは距離に関係なく入力データの任意の場所に着目できるため、長期的なパターンの認識に長けている可能性がある。
    * しかし、Transformer中の点単位のドット積自己組織化（Attention中の Query, Key Value の計算のとこなど）は、局所的な文脈の影響を受けないため、モデルが時系列の異常を受けやすくなる。
    * 本研究は、それらの課題に対してアプローチし、3つのことを実施した。
        1. Transformer アーキテクチャを時系列予測に適用することに成功し、合成データと実データの両方で大規模な実験を行い、RNN ベースのモデルよりも長期的な依存関係をうまく処理できる Transformer の潜在的な価値を検証した。
        1. self-Attention層でクエリとキーを生成するために因果的な畳み込みを採用することで、畳み込み自己記憶を提案した。
        形状などのローカルコンテキストを考慮したクエリとキーのマッチングにより、モデルの学習損失を低減し、予測精度をさらに向上させることができた。
        1. メモリのボトルネックを解消するために、わずか O(L(logL) 2 )の空間的な複雑さしか持たない LogSparse Transformer を提案した。
* Related Work
    * 時系列予測ではARIMAモデルをBox-Jenkins法で最適化したりするが、線形性を仮定したり各時系列データを独立に仮定したりするのでちょっと使いづらい。
    * 他には、時系列データを行列として取り扱い、それを因数分解することで予測を行う手法もある。
    * 階層ベイズ的なアプローチもある。
    * DNNでは、ARモデルとRNNを組み合わせる手法や、グローバルRNNを使う手法もある。
    * Transformerは自然言語処理や音声などで大きな成果を出しているが、先ほど述べた難点がありそのまま時系列データに適用することはできない。
* Background
    * この論文で取り扱うことに対して、いくつかの前提条件を定義する。
    * 互いに関連するN個の時系列データの集まりがあるとする。
        * このN個の時系列データは時間tまでの値がわかっているとする。
        * このN個の時系列データの$t+\tau$時点までの値を予測することを目標とする。
        * $t+\tau$時点までの時系列データの値はわからないが、$t+\tau$までの個々の時間はわかっているとする。
            * つまり、どの時点でデータが取得されるのかは事前に既知であるとする。
            * より単純な場合では、$t$と$t+1$の間の時間ステップは全て同じであると仮定する。
    * 予測モデルが一度学習されたあとは、予測する時はパラメータ$\Phi$はどの時点でも同じものを共有する。
    * Transformerのself-Attention機構の構造としては、multi-headのものを採用する。
        * これにより、個々のself-Attention機構が別々の短期的、長期的パターンを学習することを期待する。
    * Attentionの対象として将来の情報が選ばれることを防ぐために、softmaxの中で上三角要素をすべて-∞にするマスク行列を掛ける。
    * AttentionのFeedfowardでは2層の全結合層を使用し、活性化関数としてはReLUがつかわれる。
* Methodology
    * Enhancing the locality of Transformer
        * 時系列データの中の1点がパターンの一部なのか、変化点なのか、異常値なのかは前後の点との関係性に依存する。
        * そこで、AttentionのQVKの前に畳み込み層を入れることで、ある点と周囲の関係を利用できるようにする。
            * ある点の将来の情報を使わないようにするため、畳み込みではある点とその前の点のみを使って畳み込みを行うcausal convolutionを利用する。
            * 畳み込みのカーネルサイズ k = 1 のとき、これは通常のAttentionと同じになる。
            * QVKは別々の畳み込み層を使う。
                * そのため、QVKでそれぞれカーネルサイズを変えることも可能。
    * Breaking the memory bottleneck of Transformer