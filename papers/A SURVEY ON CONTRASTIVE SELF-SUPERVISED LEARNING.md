# A SURVEY ON CONTRASTIVE SELF-SUPERVISED LEARNING

自己教師あり学習のサーベイ。  

事前知識：
深層学習の学習方法について
* 全てラベルがついているデータでモデルを学習させる：supervised learning
* 全てラベルがついていないデータでモデルを学習させる：unsupervised learning
* 一部のラベルがついているデータと、ラベルがついていない大量のデータでモデルを学習させる：semi-supervised learning
* 一部のラベルがついているデータでモデルを学習させたのち、ラベルのついていないデータを予測させて境界領域のデータを見つけ、人間にアノテーションさせる：active learning
* 一部のラベルがついているデータと、ラベルがついていない大量ののデータでモデルを学習させる：semi-supervised learning
* 別のドメインで学習させたモデルを、ラベルつきデータが少ないドメインに適用させる：transfer learning
* 別のドメインの大量のデータと、ラベルつきデータが少ないドメインのデータを両方使ってモデルを学習させる：domain adaptation

[Semi supervised, weakly-supervised, unsupervised, and active learning](https://www.slideshare.net/ren4yu/semi-supervised-weaklysupervised-unsupervised-and-active-learning)  
[少ないアノテーションコストで楽してディープラーニングの性能を向上させる方法のまとめ](https://qiita.com/karaage0703/items/07157a0406c757ef30b8)  
[Weak Supervision: A New Programming Paradigm for Machine Learning](https://ai.stanford.edu/blog/weak-supervision/)  

論文の内容：
* ABSTRACT
    * 大規模なデータセットのアノテーションにはコストがかかるため、それを回避できる自己教師付き学習が人気を集めている。
    * 自己教師付き学習は、自分で定義した疑似ラベルを教師データとして採用し、学習した表現をいくつかの下流のタスクに使用する手法。
    * 特に最近のcontrastive learningは、コンピュータビジョンや自然言語処理（NLP）などの分野における自己教師付き学習法の主要な要素となっている。
    * contrastive learningは、データサンプルとオーグメンテーションしたサンプルは互いに近づけ、異なるサンプルは離すように潜在空間を学習させる。
    * この論文では、contrastive learningの設定で一般的に使用されるプレテクストタスクについて説明し、続いてこれまでに提案されてきた様々なアーキテクチャを紹介する。
    * 次に、画像分類、物体検出、行動認識などの複数の下流タスクについて、異なる手法の性能比較を行う。
* Introduction
    * コンピュータビジョン（CV）や自然言語処理（NLP）など、様々なタスクで深層学習は必須の技術となっている。
    * しかし、ラベル付きデータから特徴量を学習する教師付きアプローチは、何百万ものデータサンプルに手動でアノテーションを施すという多大な労力を必要とするため、ほぼ性能の限界に達している。
    * 従来の教師付き学習アプローチは、アノテーションされた学習データの量に大きく依存していた。
    * しかし、膨大な量のデータがあるにもかかわらず、アノテーションがないという問題があった。
    * そこで、高価なアノテーションを必要とせず、データ自身が教師データを提供する自己教師付き手法が深層学習の発展に重要な役割を果たしている。
    * 自己教師付き学習の手法として、GANのような生成モデルのように、自分自身のデータを再現するように学習させる手法がある。
    * しかし、GANベースの手法は、モデルが収束しなかったり識別機が強くなりすぎて生成モデルが学習できなくなったりする問題がある。
    * 生成モデルベースの手法とは異なり、contrastive learningは類似したサンプルをより近くに、それ以外のサンプルを遠くにグループ化することを目的とした識別的なアプローチである。
    * それを実現するためには、埋め込まれた潜在空間のベクトル同士の距離を図る指標が使われる。
    * contrastive learningでは、あるデータAとAをオーグメンテーションしたデータを近くに、バッチのA以外のデータを遠くに配置することで学習される。
        * そのため、手法によってはcontrastive learningでは完全に教師なしでも実施することが可能。
    * 最近の contrastive learning である SwAV、MoCo、SimCLR は教師あり学習に匹敵する性能を出している。
* Pretext Tasks
    * Pretext Taskは、疑似ラベル（pseudo labels）を用いてデータの表現を学習する自己教師付きタスクである。
    * これらの疑似ラベルは、データに含まれる性質に基づいて自動的に生成されます。
        * 色変換、幾何学的変換など。
    * Pretext Taskで学習されたモデルは、コンピュータビジョンにおける分類、セグメンテーション、検出などの下流のタスクに使用することができる。
    * さらに、理論上はこれらのPretext Taskは、画像に限らずビデオ、音声、信号など、あらゆる種類のデータに適用できる。
        * そのための必要な条件としては、適切なデータオーグメンテーションができることのような気がする。
        * 適切なデータオーグメンテーションができる -> データのラベルを変えずに類似データを生成できる手法があるということ
    * contrastive learningのプレテキストタスクでは、元画像をアンカーとし、そのオーグメンテーション（変換）版が正のサンプル、バッチまたはトレーニングデータ内の残りの画像が負のサンプルとして学習させる。
    * よく使われるのは色変換、幾何学的変換、コンテキストベースのタスク、クロスモーダルベースのタスクの4つ。
        * 色変換
            * 画像の色変換、ぼかし、色の歪み、グレースケールへの変換など。
        * 幾何学的変換
            * スケーリング、ランダムクロッピング、反転（水平、垂直）など。
        * コンテクスト・ベース
            * ジグソーパズルのように画像を分割したものの位置を入れ替えたものを正のサンプルとする手法。
            * フレームオーダーベースの手法。
                * 時系列データの場合、自身と近くのサンプルは似ていることを利用できる。
                    * 同じ動画から2つの短いシーンを抜き出して正のサンプルにする、時間的にシャッフルした動画を正のサンプルとするなど。
                    * 時間的に近いデータ、もしくは同じ動画内のデータは近いという考え方は、状態空間モデルにおける潜在空間の遷移が近いと仮定している状態と似ている気がする。
                    * 時系列データに対するcontrastive learningの適用は、深層学習を使って状態空間モデル的なものを作ろうとしているのかもしれない。
        * 
