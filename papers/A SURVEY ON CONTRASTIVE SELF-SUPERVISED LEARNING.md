# A SURVEY ON CONTRASTIVE SELF-SUPERVISED LEARNING
[A SURVEY ON CONTRASTIVE SELF-SUPERVISED LEARNING](https://arxiv.org/abs/2011.00362)  

自己教師あり学習のサーベイ。
参考：[自己教師あり学習などの学習の種類](http://wiki.waishimu.com:8080/user/yshim/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/%E8%87%AA%E5%B7%B1%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92%E3%81%AA%E3%81%A9%E3%81%AE%E5%AD%A6%E7%BF%92%E3%81%AE%E7%A8%AE%E9%A1%9E)

* ABSTRACT
    * 大規模なデータセットのアノテーションにはコストがかかるため、それを回避できる自己教師付き学習が人気を集めている。
    * 自己教師付き学習は、自分で定義した疑似ラベルを教師データとして採用し、学習した表現をいくつかの下流のタスクに使用する手法。
    * 特に最近のcontrastive learningは、コンピュータビジョンや自然言語処理（NLP）などの分野における自己教師付き学習法の主要な要素となっている。
    * contrastive learningは、データサンプルとオーグメンテーションしたサンプルは互いに近づけ、異なるサンプルは離すように潜在空間を学習させる。
    * この論文では、contrastive learningの設定で一般的に使用されるプレテクストタスクについて説明し、続いてこれまでに提案されてきた様々なアーキテクチャを紹介する。
    * 次に、画像分類、物体検出、行動認識などの複数の下流タスクについて、異なる手法の性能比較を行う。
* Introduction
    * コンピュータビジョン（CV）や自然言語処理（NLP）など、様々なタスクで深層学習は必須の技術となっている。
    * しかし、ラベル付きデータから特徴量を学習する教師付きアプローチは、何百万ものデータサンプルに手動でアノテーションを施すという多大な労力を必要とするため、ほぼ性能の限界に達している。
    * 従来の教師付き学習アプローチは、アノテーションされた学習データの量に大きく依存していた。
    * しかし、膨大な量のデータがあるにもかかわらず、アノテーションがないという問題があった。
    * そこで、高価なアノテーションを必要とせず、データ自身が教師データを提供する自己教師付き手法が深層学習の発展に重要な役割を果たしている。
    * 自己教師付き学習の手法として、GANのような生成モデルのように、自分自身のデータを再現するように学習させる手法がある。
    * しかし、GANベースの手法は、モデルが収束しなかったり識別機が強くなりすぎて生成モデルが学習できなくなったりする問題がある。
    * 生成モデルベースの手法とは異なり、contrastive learningは類似したサンプルをより近くに、それ以外のサンプルを遠くにグループ化することを目的とした識別的なアプローチである。
    * それを実現するためには、埋め込まれた潜在空間のベクトル同士の距離を図る指標が使われる。
    * contrastive learningでは、あるデータAとAをオーグメンテーションしたデータを近くに、バッチのA以外のデータを遠くに配置することで学習される。
        * そのため、手法によってはcontrastive learningでは完全に教師なしでも実施することが可能。
    * 最近の contrastive learning である SwAV、MoCo、SimCLR は教師あり学習に匹敵する性能を出している。
* Pretext Tasks
    * Pretext Taskは、疑似ラベル（pseudo labels）を用いてデータの表現を学習する自己教師付きタスクである。
    * これらの疑似ラベルは、データに含まれる性質に基づいて自動的に生成されます。
        * 色変換、幾何学的変換など。
    * Pretext Taskで学習されたモデルは、コンピュータビジョンにおける分類、セグメンテーション、検出などの下流のタスクに使用することができる。
    * さらに、理論上はこれらのPretext Taskは、画像に限らずビデオ、音声、信号など、あらゆる種類のデータに適用できる。
        * そのための必要な条件としては、適切なデータオーグメンテーションができることのような気がする。
        * 適切なデータオーグメンテーションができる -> データのラベルを変えずに類似データを生成できる手法があるということ
    * contrastive learningのプレテキストタスクでは、元画像をアンカーとし、そのオーグメンテーション（変換）版が正のサンプル、バッチまたはトレーニングデータ内の残りの画像が負のサンプルとして学習させる。
    * よく使われるのは色変換、幾何学的変換、コンテキストベースのタスク、クロスモーダルベースのタスクの4つ。
        * 色変換
            * 画像の色変換、ぼかし、色の歪み、グレースケールへの変換など。
        * 幾何学的変換
            * スケーリング、ランダムクロッピング、反転（水平、垂直）など。
        * コンテクスト・ベース
            * ジグソーパズルのように画像を分割したものの位置を入れ替えたものを正のサンプルとする手法。
            * フレームオーダーベースの手法。
                * 時系列データの場合、自身と近くのサンプルは似ていることを利用できる。
                    * 同じ動画から2つの短いシーンを抜き出して正のサンプルにする、時間的にシャッフルした動画を正のサンプルとするなど。
                    * 時間的に近いデータ、もしくは同じ動画内のデータは近いという考え方は、状態空間モデルにおける潜在空間の遷移が近いと仮定している状態と似ている気がする。
                    * 時系列データに対するcontrastive learningの適用は、深層学習を使って状態空間モデル的なものを作ろうとしているのかもしれない。
        * クロスモーダルベース
            * 同じ場所を同じ時間に複数の視点から撮影した画像、動画を使用する。
            * 別の時間に撮影した画像とは離れている方が好ましい。
    * 適切なPretext Taskの選定
        * Pretext Taskの選択は、解決する問題のタイプに依存する。
        * これまでの研究により、contrastive learningでモデルがうまく機能するためには、適切な種類のPretext Taskを使用することが重要であることが明らかになっている。
        * Pretext Taskの主な目的は、他のデータポイントに対する識別性を保ちつつ、変換に対してモデルが不変であることを強制すること。
        * しかし、このような拡張によってもたらされるバイアスは諸刃の剣となる可能性がある。
        * というのも、それぞれの拡張は、ある場合には有益であり、ある場合には有害である変換に対する不変性を促すため。
        * 例えば、回転を適用すると、視野に依存しない空中での画像認識には役立つかもしれないが、ディスプレイ用途の写真でどちらが上かを検出するといった下流のタスクを解決しようとすると、性能が大幅に低下する可能性がある。
        * 同様に、色付けに基づくプレテクストタスクは、図 9 で表されるような細かい分類ではうまくいかないかもしれない。
        * 回転を除き、スケーリングやアスペクト比の変更などの他の変換は、容易に検出可能な視覚的アーティファクトを生成するため、Pretext Taskには適していない可能性があると指摘する論文もある。
        * また、その論文では図 10 に示すように、ターゲットデータセットの画像が、DTD データセットのようにカラーテクスチャで構成されている場合、回転はうまく機能しないことも明らかにしている。
    * NLP におけるPretext Task
        * NLPでのPretext Taskとは、教師ありのアプローチを教師なしの問題に適用して、モデルを事前学習できるようなラベルを生成することを指す。
        * NLPの方が昔からcontrastive learning的な手法をいろいろ試している感じがする。
        * Center and Neighbor Word Prediction
            * Word2Vecもcontrastive learningの一種。
            * アンカーとなる元データは欠損した単語、正例のデータは周囲の単語、負例のデータはランダムにサンプルされてきた単語、という認識（間違ってるかも）。
            * "center word prediction"では、モデルへの入力は固定の窓サイズを持つ単語のシーケンスで、シーケンスの中心から 1 つの単語が欠けている。
            * モデルのタスクは、単語列の中で欠けている単語を予測すること。
            * "neighbor word prediction"の入力は 1 つの単語で、モデルはその隣接する単語を予測する。
        * Next and Neighbor Sentence Prediction
            * Next Sentence Prediction では 2 つの入力文が連続した文になるかどうかをモデルが予測する。
            * この場合の正のサンプルは元の文に続くサンプルであり、負のサンプルはランダムな文書からの文。
            * 文が与えられたときに、その前の文と次の文を予測するのが「Neighbor Sentence Prediction」。
            * これはNeighbor Word Predictionに似ているが、単語の代わりに文に適用される。
        * Auto-regressive Language Modeling
            * 前の単語が与えられたときに次の単語を予測したり、その逆を行ったりする。
            * この手法は、GPT[32]やその最近のバージョンなど、いくつかの N-gram モデルやニューラルネットワークで使用されている。
        * Sentence Permutation
            * BART では、コーパスから連続したテキストを取り出し、複数の文に分割するPretext Taskを使用している。
            * 文章の位置はランダムに入れ替えられ、モデルのタスクは文章の元の順序を予測すること。
* Architectures
    * contrastive learningでは良質な表現を生成するために、負のサンプルの数が重要となる。
    * これは、辞書を探すタスクと見なすことができ、辞書はトレーニングセット全体の場合もあれば、データセットの一部のサブセットの場合もある。
    * この負のサンプルを探す手法をEnd-to-End、Memory Bank、Momentum Encoder、Clusteringの4種類に分類した。
    * End-to-End
        * アンカーになる元データの埋め込み表現を学習させるクエリエンコーダ（Q）と、正例、負例の埋め込み表現を学習させるキーエンコーダ（K）の2つを作り、両方ともバックプロパゲーションで更新される。
        * contrastive lossを使って、正のサンプルは元のサンプルに近づけ，負のサンプルは遠ざけるように学習される。
        * 類似性指標としてコサイン類似度が使われる。
        * 図 12 に示すように、バッチサイズが大きく、エポック数が多いほど性能が向上することが確認されている。
        * 強力な自己回帰モデルとコントラスト損失を用いて、潜在空間で未来を予測することにより、高次元時系列データの特徴表現を学習することもできるらしい。
        * End-to-End な手法の難点としては、バッチサイズは GPU のメモリサイズによって制限されるため、GPUがボトルネックになる点がある。
        * ミニバッチ最適化の問題も出てくるらしい。
    * Memory Bank
        * バッチサイズが大きくなると最適化の問題が出てくるので、メモリバンクと呼ばれる別の辞書を保持することで解決する手法。
        * 最適化の問題とは？
            * キーエンコーダ の gradient をバックプロパゲーションする時にコストが大きくてGPU依存になるとかかな。
        * 過去数エポック分のサンプルの埋め込みの指数的移動平均を辞書として保存して使うらしい。
        * 表現が数回のパスですぐに古くなってしまうため、メモリバンクの表現を更新するのに計算量がかかることが難点。
    * Momentum Encoder
        * 上記のメモリーバンクの難点を改善するため、メモリバンクの代わりに「モーメンタムエンコーダ」を使用する手法がある。
        * keyのモーメンタムエンコーダはバックプロパゲーションされない。
        * 代わりに、keyのパラメータ θ_k は、queryのパラメータ θ_q とmomentum coefficient の m を使って以下のように更新される。
        * θ_k ← m \* θ_k + (1 − m) \* θ_q
        * これにより、keyのエンコーダの最適化の問題がなくなるらしい。
    * Clustering
        * End-to-Endなアプローチに、似ている埋め込み表現を持つサンプル群がクラスタを作るような処理を入れたもの。
        * クラスタ数のハイパーパラメータをどうやって決めるのかは謎。
            * 想定されるラベル数とかを基準にするのだろうか。
            * weakly supervisedな雰囲気を感じる。
* Encoders
    * 
