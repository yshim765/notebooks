# Clustering and Classification for Time Series Data in Visual Analytics: A Survey

* ABSTRACT
    * Visual analytics って何？
        * たぶん、可視化して何らかの洞察を得ることかな
    * 基礎となるプロセスの探求、トレンドの特定と推定、意思決定、未来の予測あたりを目的として様々な手法が開発されてきた。
    * このサーベイでは、機械学習のように自動でデータから情報を抽出する手法だけでなく、インタラクティブな探索法も取り扱っている。
        * これは、機械学習の結果をインタラクティブな可視化システムに組み込むことは効果が大きいから。
    * このサーベイでは過去の60の論文を6つの視点からまとめた。
* I. INTRODUCTION AND MOTIVATION
    * INTRODUCTION AND MOTIVATION
        * 時系列データはネットワークやシステム、気象学、ソーシャルメディア、行動分析、軌跡データ、生物学、金融など、様々な分野で使われている。
        * 時系列データの定義は「an ordered collection of observations or sequence of data points made through time at often uniform time intervals」
            * 「（多くの場合一定の時間間隔で）時間を通して得られ、順序を持った、データの配列や観察データのコレクション」
        * このサーベイでは、時系列データをその構造を元に4つのカテゴリに分類した。
            1. univariate : 1変量
            1. multivariate : 多変量
            1. tensor fields : [テンソル場](https://ja.wikipedia.org/wiki/%E3%83%86%E3%83%B3%E3%82%BD%E3%83%AB%E5%A0%B4)
            1. multifield : マルチフィールド（これはよくわからない）
            * ココらへんの内容はセクション2で説明されている
        * 機械学習は「optimizing a performance criterion using example data and past experience」と言われている。
            * 「例示されたデータと過去の経験を元に性能基準を最適化すること」
        * 機械学習は教師あり学習と教師なし学習がある
            * 教師あり学習：学習データとして入力データと出力となる正解ラベルのペアを使用し、入力と出力を対応付ける関数を生成することを目的とする
            * 教師なし学習：学習データとしてラベルのない入力データのみを使用し、データに隠されたパターンや実質的な構造を見つけることを目的とする
        * このサーベイでは、時系列データを対象とした教師あり学習として分類を、教師なし学習としてクラスタリングに焦点を当てている
        * Sachaらは機械学習の利点を2つ挙げている
            1. 非構造化データを人間が探索、分析、理解しやすい形に変換すること
            1. 教師なしまたは半教師ありのアルゴリズムを使うことで、最適なビジュアライゼーション、検証、探索のステップの連続性などを提案し、分析自体を指示すること。また、そうすることでアルゴリズムが生データから複雑なパターンを直接自動的に発見すること
        * 機械学習の研究ではアルゴリズムに重点が置かれているのに対し、インタラクティブな可視化の研究では人間がわかりやすい可視化をすることに重点が置かれてきた
        * この違いはユーザの役割の違いを示している
            * 機械学習はユーザの介入をなくし、全て自動化することを目的としている
            * 可視化はユーザ自身が積極的に分析するための補助ツールの開発を目的としている
        * visual analytics の目的は生データから知見を得ること
            * そのために、機械学習で得られた高度な分析結果をインタラクティブに可視化することを目指している
    * A. SURVEY SCOPE AND INTENDED AUDIENCE
        * 時系列データのクラスタリングと分類は2つの異なる分野から出発してきた
            1. データマイニングの分野
                * いろいろ先行の調査はあるが、アルゴリズムが中心でユーザの影響（とは？）はあまり重視されていない
            1. 可視化の分野
                * いろいろ先行の調査はあるが、機械学習と可視化の組み合わせに焦点をおいた研究はあまりないので、我々はそこにフォーカスしている
        * 機械学習とインタラクティブな可視化の統合は両分野から求められて推進され、いろいろと研究されている
        * 時系列データの持つ以下の特徴が原因で、クラスタリングや分類が難しくなっている
            * ノイズの存在
            * 高次元であること
            * 特徴の相関性が高いこと
        * 時系列データの分析において問題になりがちなこと
            * 多くの手法が特徴をベクトルで受け取ることしかできないが、時系列データには明確な特徴がない場合が多いこと
            * 特徴空間の次元が高く、計算コストが高いこと
            * 明示的な特徴がないため、解釈可能な手法を構築するとの負担が大きいこと
        * 2つのデータの類似性を計算する時に、時系列データと非時系列データの違いが大きく現れる
            * 時系列データ特有の特性として、外れ値、系列のシフトなどのノイズ、時系列自体の長さの変化などがある
        * 可視化の文脈では、分類とクラスタリングは、後の解釈のためにデータを抽象化するという共通の目標があると言える
        * visual analyticsでは分析したあとにユーザが知りたい性質について取捨選択することが必要である
        * このサーベイでは、調査対象となった論文を次の6つの観点で分類した。
            1. 時系列データの構造
            1. 時系列データの類似性尺度と特徴抽出
            1. 時系列解析技術
                1. クラスタリング
                1. 分類
            1. 可視化解析
            1. 評価手法
        * データから有用な情報を抽出するという目的で使われる手法として、時系列解析では統計モデリング（ARMA, ARIMAなど）、k-means、SVMなどがある。ニューラルネットワークを使用したものとしてはRNNもつかわれている。
            * ちょっと手法が古いかも
        * 時系列データに対するタスクで分類やクラスタリング以外のものとしては以下のものがある
            * 検索、パターン抽出
            * 将来の数値予測
                * ARIMAやBox-Jenkins法の他に、ニューラルネットワークのRNNやLSTMなどもつかわれている
        * 選んだ論文は直近13年くらい
        * 今回は自然言語処理的な研究は対象外としている
        * 本サーベイの構成は以下の通り
            * セクション2 : 時系列データの構造
            * セクション3 : 時系列データの類似性の測定と特徴抽出
            * セクション4 : 時系列データのクラスタリングと分類につかわれるアルゴリズム
            * セクション5 : 時系列データの可視化手法について
            * セクション6 : これらの手法の評価方法について
        * 
* II. TIME SERIES DATA STRUCTURES
    * このサーベイでは時系列データを4種類に分ける。
    * 今回のサーベイで取り扱っている論文では、多変量とテンソル場のデータを対象にしたものが多い。
        1. univariate : 1変量
            * 一変量時系列とは，1つの時間ごとに1つのデータ値のみを含む系列である。
            * ある期間にわたる都市の気温とか。
        1. multivariate : 多変量
            * 多変量時系列は、同じタイムスタンプを持つ時系列の集合である。
            * 温度や圧力の測定値のように、時間を通じて捕捉された複数の一変量の集合であったりする。
            * 3軸加速度計から測定された3次元加速度のように、多変量の各構成要素が同じ単位とセンサソースを持つような連想的な多変量の場合もある。
            * 多くの論文で取り扱われている
            * 生物学、医学、金融、アニメーション、製造システムや予測保全など様々な分野で扱われる。
        1. tensor fields : [テンソル場]
            * tensor fieldsは、可変の軸を持つ規則的なグリッド上に配置されたデータの配列である。
            * 空間のグリッド状の各点がそれぞれデータを持ち、それの時系列的変化をデータとして残すとtensor fieldsの時系列データになる。
            * 日本各地の毎日の風向きの変化とかtensor fieldsになりそう。
            * グラフやネットワークの時系列、移動する物体の空間的位置の時系列、空間的構成／分布の時系列など。
            * tensor fieldsの1点で測定されたデータ1つの集合が1変量データになるのかな
            * グラフとネットワークの時系列
                * グラフのノードやエッジのデータの時間的変化は時系列データになる
                * 1つのコンピュータを1つの点として、コンピュータ同士のつながりをグラフにして解析するとか
            * 移動する物体の空間的位置の時系列データ
                * 移動体の空間的な位置情報に時間成分を加えたデータはトラジェクトデータと呼ばれる
                * 車両や航空機などの多様な移動体の移動性を理解、認識して、経路発見、移動分析、位置予測につなげることができる
            * 空間構成と分布の時系列データ
                * 行動パターンの発見や、特定の自治体や公共・ビジネスセクターで発生する可能性のある興味深いイベントの発見を、空間的な構成や分布とみなすことができる
                * 時間経過に伴う規則的な構成と分布の特定は、選択した空間スケールから抽出したイベントと行動の総数で表される。
                * 個人の移動行動と移動パターン、動物の行動、気候（気象）とオゾン層のパターン変化、およびしばしば一定の時間間隔で時間をかけて作成された行動キャプチャデータなど。
        1. multifield : マルチフィールド
            * tensor fieldsのデータの集合体。
* III. SIMILARITY MEASURES AND FEATURE EXTRACTION FOR TIME SERIES DATA
    * A.生データの類似性
        * 類似性指標は、どのようなことを以て似ていると判断したいかによって用いる手法が異なると思っている（私見）
            * たとえば、周期がにているから、長さが似ているから、大きさが似ているから、など
        * ahyaoui and Al-MutairiおよびWangらは、次の4つの主要なカテゴリに分類している。
            1. lock-stepな尺度
                * ユークリッド距離やマンハッタン距離など、同じ時間のデータ同士、もしくは同じ時間だけずれたデータ同士を比べる。
                * ユークリッド距離：差の合計を二乗したものの平方根。同じ時間の（もしくは、同じ時間だけずれた）データ同士を比べる。
            1. 弾性的な尺度
                * 最長共通部分配列やDynamic Time Warping (DTW)など、別の任意の時間どうしを比べることができる指標。
                * Dynamic Time Warping (DTW)：距離を計算する前に時系列を整列させる感じの手法。
                    * [これとかわかりやすい](https://data-analysis-stats.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/dtwdynamic-time-warping%E5%8B%95%E7%9A%84%E6%99%82%E9%96%93%E4%BC%B8%E7%B8%AE%E6%B3%95/)
                    * 局所的な構造が全く異なる2つの時系列が、DTWによって誤ってマッチングされてしまう可能性があるが、整列アルゴリズムを変えることで改善が可能らしい。
                    * 計算コストは系列長の二乗になるので、やや大変。
                        * ただ、それは計算方法を工夫したら割となんとかなるらしい（噂
                * 相関（相互相関）：2つの時系列感の相関。時間的にずらして相関を計算するとラグがある場合でも関係性が見えたりする。
                    * 自分自身のラグと相関を取るのは自己相関と呼ばれ、周期成分のラグを知りたいときとかに使う。
            1. パターンベースの尺度
                * spatial assembling distance [SpADe]とかあるらしい。
                * このサーベイでは触れない。
            1. 閾値ベースの尺度
                * threshold query based similarity search [TQuEST]とかあるらしい。
                * このサーベイでは触れない。
    * B.特徴抽出
        * 特徴抽出は次元削減の一形態であり、高次元データを扱う際の計算コストを下げ、クラスタリングや分類の精度を高めるのに役立つ
        * 離散フーリエ変換（DFT）、離散ウェーブレット変換（DWT）、離散コサイン変換（DCT）、特異値分解（SVD）、Adaptive Piecewise Constant Approximation（APCA）、Piecewise Aggregate Approximation（PAA）、チェビシェフ多項式（CHEB）、Symbolic Aggregate approXimation（SAX）など、いろいろある。
        * 主成分分析
            * データの分散が最も大きくなる新しい軸を探すことで次元圧縮を行う
        * [Multidimensional Scaling（MDS）](https://www.macromill.com/service/data_analysis/multi-dimensional-scaling.html)
            * 距離行列を作り、それを元にデータを配置する新しい軸を作る手法
            * 距離行列がわかっているならk-meansとかでいいのでは？？？
            * 時系列中の連続したデータを取り出したものを[n-gram](https://qiita.com/kazmaw/items/4df328cba6429ec210fb#:~:text=%E4%BB%BB%E6%84%8F%E3%81%AE%E6%96%87%E5%AD%97%E5%88%97%E3%82%84,tri%2Dgram%EF%BC%89%E3%81%A8%E5%91%BC%E3%81%B6%EF%BC%8E)で表すことがある
                * 2つのbigram、3つのtrigramは聞いたことある。
        * 離散フーリエ変換
            * 時間周波数に直す方法。
        * 離散ウェーブレット変換
            * 周波数の代わりに[ウェーブレット](https://www.mnc.toho-u.ac.jp/v-lab/yobology/wavelet/wavelet.htm)に直す方法。
            * [shaplet](https://tslearn.readthedocs.io/en/stable/user_guide/shapelets.html)に直す方法もある。
* IV. TIME SERIES ANALYSIS TECHNIQUES
    * A.クラスタリング
        * ラベル付サれていないデータから、似ているものはどれかを探し出すことを目的とする
        * 時系列データでは特異な構造（高次元、ノイズ、高い特徴相関など）があるので、特殊な方法が編み出されてきた。
        * 時系列のクラスタリングは大きく3つに分類される。
            1. 時系列全体のクラスタリング
            1. 部分時系列のクラスタリング
                * 長い時系列の1部分を取り出し、全体の中で似た箇所がないかを探す
            1. 時系列中の1点を対象としたクラスタリング
        * 見やすいので可視化には散布図がつかわれがち
        * 手法は大きく5つに分類される
            1. パーティショニング法
                * ラベルのないデータをk個のグループに分割する手法。
                * k-Means（KM）、k-Medoids（PAM）、Fuzzy c-Means（FCM）、Fuzzy c-Medoidsなど。
                * これらは大きくハードクラスタリングとソフトクラスタリングの2種類に分かれる。
                    1. ハードクラスタリング
                        * ハードクラスタリング手法では、各オブジェクトは1つのクラスタにしか割り当てられない。
                        * k-Means（KM）
                            * K個のクラスター中心をランダムに初期化し、各例を最も近いクラスターに割り当て、クラスター中心の位置を再計算するのを繰り返す。
                        * [k-Medoids（PAM）](http://ibisforest.org/index.php?k-medoids%E6%B3%95)
                            * クラスター中心の位置を計算するのではなく、代表データを決めることでk-means的なクラスタリングを行う。
                            * データの位置や距離が計算できる形で与えられない場合でも、データ間の距離行列が定義できれば計算可能。
                            * 距離行列をDTWで計算し、k-Medoidsを適用するとかやるらしい。
                    1. ソフトクラスタリング
                        * 各オブジェクトは確率的に複数のクラスタに割り当てられる。
                        * 可視化が難しい。
                        * [Fuzzy c-Means（FCM）](http://ibisforest.org/index.php?%E3%83%95%E3%82%A1%E3%82%B8%E3%82%A3c-means%E6%B3%95)
                            * k-meansの計算で、各データは各クラスターからの距離を元に重みを持って割り当てられる。
                            * クラスタの再計算は、各データに各クラスターの重み係数をかけて計算される。
                        * Fuzzy k-Medoids
                            * Fuzzyなk-Medoids。
                * パーティショニング法では、クラスタ数は予め決めておく必要がある。
                * また、長さが不定の時系列データには不向き。
            1. 階層化法
                * 樹形図を作っていく方法。
                * 可視化手法としてわかりやすいので、ラインプロット、ヒートマップと一緒によくつかわれるらしい。
                * 凝集型アルゴリズム（ボトムアップ）
                    * 最も近いデータ（もしくはクラスター）2つをくっつけて1つのクラスターにすることを繰り返す。
                    * それを続けると、ある時に最も近いデータ（もしくはクラスター）間の距離が大きくなる瞬間が来るので、そこで止めると良い感じのまとまりができる。
                    * クラスター間の距離の測り方はいくつかある。
                        * 単純連結法(single-link method)
                            * 各々のクラスターの中で最も近いベクトル同士の距離をそのクラスター同士の距離とする方法。
                        * 完全連結法(complete-link method)
                            * 各々のクラスターの中で最も遠いベクトル同士の距離をそのクラスター同士の距離とする方法。
                        * 重心法(centroid)
                            * 各々のクラスターの重心ベクトル同士の距離をそのクラスター同士の距離とする方法。
                        * ウォード法(centroid)
                            * 2つのクラスターを統合する際に、クラスター内の重心から各サンプルへの距離の平方和が最小となるようにクラスターを形成する手法。
                    * [参考](https://qiita.com/g-k/items/8f0d9905d3e106caed59)                
                * 分割アルゴリズム（トップダウン）
                    * 最初にデータ全体を1つのクラスタとし、それを2つずつに分割していく方法。
            1. モデルベース法
                * 自己組織化マップ(SOM)。
                    1. 二次元状のマップを用意し、マップの各点にデータと同じ構造のノードを用意する。
                    1. ノードは乱数で初期化する。
                    1. 学習データを1つ用意し、その学習データに最も近いノードを探し出し、その点を学習データと平均した値に更新する。
                    1. ノードの周囲の点も同様に更新する。
                    1. これを繰り返すと、似たデータ構造が近くに集まるマップができる。
                    1. 新しいデータがどのノードに一番近いかを見ると、どのクラスタに近いかとかもわかる。
                * データの近さがわかる、クラス多数を決めなくてもいい、クラスタの境界にあるデータも境界にあることがわかる、あたりが利点かな。
                * [参考](http://gaya.jp/spiking_neuron/som.htm)
            1. グリッドベース法
                * 各データを適当な空間に配置し、その空間を格子状に分割し、データ数の多い格子を中心として周囲の格子を吸収していくことでクラスターを作っていく。
                * 計算速度が速いのが利点らしい。
                * あと、クラス多数を指定しなくていいも良い。
            1. 密度ベース法
                * データの周囲r以内の距離に他のデータが何個あるかを数え、その数が一定以上のデータ（コア点）とコア点につながっているデータをクラスタとして分類する。
                * 周囲にデータがない点は異常値として分類できるのが良い。
                * クラスタ数の指定が要らないのも良い。
                * パラメータrはいいとして、コア点を決定するためのしきい値はデータ数に依存しそうなので、データが増えると適宜調整がいいりそうなのは微妙かも。
                * ただ、結果はかなり良い感じなので、とりあえず最初に試したいときには使ってみたい。
    * B.分類
        * k近傍法、決定木、ランダムフォレスト、SVM、ニューラルネットワークなど。
        * 実はk近傍法やSVMは適切な距離が出せれば普通に使えるという説も。
        * ニューラルネットワークは1次元CNNとか。
        * 短時間フーリエ変換してから2次元CNNにかけるのは、1次元の波を周波数成分ごとに分ける特徴抽出を行っていると考えると、1次元にそのまま突っ込むよりNNが学習しやすいのかもしれない。
* V. VISUAL ANALYSIS
    * あんまり面白いこと書いてない。
* VI. EVALUATION APPROACHES
    * あんまり面白いこと書いてない。
* VII. INTEGRATION OF VISUALIZATION AND ANALYSIS TECHNIQUES
    * あんまり面白いこと書いてない。
* VIII. CONCLUSION
    * あんまり面白いこと書いてない。
